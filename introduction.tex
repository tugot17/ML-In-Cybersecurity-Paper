Machine learning (ML) and especially its sub-domain Deep Learning (DL) are technologies that in recent years deeply have changed the world [1]. Except pure academic usage DL and ML solutions are widely used in the industry and what is especially important for us, in the biometric systems. Unfortunately how recent studies shown DL methods are vulnerable to adversarial attacks [8,9]. This means that these machine learning models incorrectly classify examples that are only slightly different from correctly classified examples from data distribution. Since ML and DL algorithms are widely used in security areas (such as biometrics) it is extremely important to understand how such attacks work, why they work and how to defend against them. 

Authors of 'Real and Stealthy Attacks on State-of-the-Art Face Recognition' [2] paper came up with an idea of systematic method to automatically generate attacks on Face Recognition systems using a printed pair of eyeglass frames. When the attacker is wearing such eyeglass frames he/she is not classified as himself/herself by a Face Recognition System. As a benchmark Face Recognition System (FRS) authors have used  'Deep-Face-Recognition' [6] - state of the art algorithm for Face Recognition problems in 2016 (when Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition was first published). They show how to generate such eye-frames, what problems may occur during generation or printing and how to avoid such issues.
